# LLM Agent + Python Game Engine Integration Framework

## A Dual-Representation Architecture for ARC-AGI-3 Domain Modeling

*Integrating the LearningAgent's Structured Natural Language Pipeline with an Executable Python Symbolic Model*

February 2026

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [The LearningAgent: What Exists Today](#2-the-learningagent-what-exists-today)
3. [The Problem: Why NL-JSON Alone Is Insufficient](#3-the-problem-why-nl-json-alone-is-insufficient)
4. [Proposed Architecture: Dual-Representation Integration](#4-proposed-architecture-dual-representation-integration)
5. [Python Game Engine Design](#5-python-game-engine-design)
6. [Integration with the LearningAgent Lifecycle](#6-integration-with-the-learningagent-lifecycle)
7. [The Compilation Step: NL-JSON to Python Model](#7-the-compilation-step-nl-json-to-python-model)
8. [Verification Harness and Feedback Loop](#8-verification-harness-and-feedback-loop)
9. [Concrete Data Flow: A Worked Example](#9-concrete-data-flow-a-worked-example)
10. [Implementation Roadmap](#10-implementation-roadmap)
11. [Why Python Is the Right Modeling Language](#11-why-python-is-the-right-modeling-language)
12. [References](#12-references)

---

## 1. Executive Summary

The **LearningAgent** (`agents/templates/learning_agent/`) is a custom LLM-driven agent that discovers ARC-AGI-3 game mechanics from scratch. It operates through a multi-phase loop: take an action, observe before/during/after frames, analyze the transition with an LLM, and progressively build structured natural language (NL-JSON) descriptions of objects, actions, environment rules, and constraints. Three independent LLM calls per cycle handle action analysis, environment analysis, and next-action suggestion respectively.

This framework proposes **integrating an executable Python game engine** alongside the existing NL-JSON model. The Python model is a per-game simulation — generated by the LLM from the NL-JSON state — that can be **run forward to predict outcomes** and **compared against actual ARC-AGI-3 API observations**. When predictions match reality, the model is validated. When they diverge, the structured diff between predicted and actual states produces a precise error signal that is fed back into the LLM's NL-JSON refinement loop.

The key insight: the LearningAgent already has the architecture to support this. Its `ActionKnowledge` tracks per-action definitions and consistency. Its `EnvironmentKnowledge` tracks objects, constraints, and spatial rules. Its verification system counts consecutive consistent observations. The Python game engine extends this by making the entire model **executable and testable** — converting "3 consistent LLM opinions" into "3 correct forward predictions against ground truth."

---

## 2. The LearningAgent: What Exists Today

### 2.1 Core Architecture

The LearningAgent is a zero-prior-knowledge agent that discovers game mechanics through systematic experimentation. Its architecture separates concerns across distinct modules:

```
LearningAgent (agent.py)          ← Orchestration: 5-phase action loop
├── LLMAgent (llm_agents.py)      ← Three independent LLM calls per cycle
├── KnowledgeManager (knowledge.py) ← State persistence, formatting, updates
├── FrameDiffer (diff.py)          ← Pixel + object-level frame comparison
├── FrameCapture (vision.py)       ← Grid rendering, base64 encoding
├── ObjectDetector (object_detection.py) ← Gestalt grouping, movement tracking
└── RunLogger (run_logger.py)      ← Comprehensive logging of every call
```

### 2.2 The Multi-Phase Action Loop

Each cycle of the agent (`choose_action` in `agent.py:125-303`) executes up to five phases:

| Phase | What Happens | LLM Call? |
|-------|-------------|-----------|
| **1. Analyze Previous Action** | Compare before/after frames, compute diffs, call LLM for interpretation | Yes: `analyze_action` |
| **1.5. Analyze Environment** | Dedicated call to refine world understanding based on latest evidence | Yes: `analyze_environment` |
| **2. Continue Setup Sequence** | Execute pre-planned verified actions to change game state | No |
| **3. Get Next Action Suggestion** | Ask LLM what to test next, with what setup | Yes: `suggest_next_action` |
| **4/5. Execute Action** | Capture before-frame, execute via API, store for next-cycle analysis | No |

### 2.3 The NL-JSON State Model

The agent maintains its understanding in a fully serializable `AgentState` (Pydantic model, `models.py:373-507`):

**Per-Action Knowledge** (`ActionKnowledge`, `models.py:58-121`):
```
{
  "action_id": "ACTION1",
  "current_definition": "Moves the player up, blocked by grey walls and border",
  "observations": [
    {
      "before_frame_path": "frames/before_0004.png",
      "after_frame_path": "frames/after_0005.png",
      "diff_summary": "Total: 32 pixels changed. Regions: upper_left_playarea (32)",
      "llm_interpretation": "Player object (color 2) moved up by 4 pixels...",
      "had_effect": true,
      "was_consistent": true,
      "context_that_caused_outcome": "Player was in open space with no wall above",
      "object_changes": "MOVED: Color 2 block moved up"
    }
  ],
  "verification_attempts": 5,
  "is_verified": true,
  "consecutive_no_effects": 0,
  "is_exhausted": false
}
```

**Environment Knowledge** (`EnvironmentKnowledge`, `models.py:144-201`):
```
{
  "background_color": "Color 0 (black)",
  "has_border": true,
  "border_color": "Color 5 (grey)",
  "internal_walls": ["Grey blocks at rows 20-24 form a horizontal wall"],
  "movement_constraints": ["Player blocked by grey walls", "Border prevents edge movement"],
  "identified_objects": [
    {"name": "player", "color": "2 (red)", "shape": "4x4 block",
     "role_hypothesis": "Player character", "evidence_for_role": "Responds to ACTION1-4"}
  ],
  "spatial_structure": "64x64 grid with grey border, internal maze walls",
  "domain_description": "Sokoban-like puzzle: move player to push boxes onto targets",
  "breakthroughs": ["Grey walls are impassable", "Yellow squares are push targets"]
}
```

### 2.4 Verification and Consistency

The existing agent verifies action understanding through observation consistency:

- **Verified**: 3 consecutive observations where the LLM marks `is_consistent_with_previous = true` (`models.py:97-100`)
- **Exhausted**: 8 consecutive no-effect observations (`models.py:86-95`)
- **Hard guards**: Actions that produce no effect are blocked until a state change occurs (`agent.py:657-746`)

This is a **soft verification** — it measures whether the LLM's *descriptions* are internally consistent, not whether they correctly predict behavior.

### 2.5 What the Agent Does Well

1. **Evidence-based discovery**: All knowledge comes from observed transitions, not assumptions
2. **Structured state management**: Full Pydantic serialization enables persistence and debugging
3. **Separation of concerns**: Three LLM calls prevent monolithic reasoning
4. **Object-level reasoning**: Gestalt grouping lifts pixel diffs to "player moved left"
5. **Loop prevention**: Hard guards and exhaustion detection prevent infinite no-effect loops
6. **Stage tracking**: Detects level transitions via score changes and resets assumptions

---

## 3. The Problem: Why NL-JSON Alone Is Insufficient

### 3.1 Descriptions Are Not Predictions

The current `ActionKnowledge.current_definition` is a natural language string like:

> "ACTION1 moves the player up by one tile. If there is a grey wall above the player, the player does not move. If there is a pushable box above the player and empty space above the box, both the player and box move up."

This description may be correct, but it cannot be **executed**. The agent cannot:
- Predict what the board will look like after ACTION1 from a given state
- Verify whether its understanding is correct without asking the LLM again
- Detect which *specific part* of its understanding is wrong when an observation contradicts it

### 3.2 Consistency ≠ Correctness

The current verification counts "3 consistent LLM opinions." But the LLM may consistently produce the same wrong interpretation. Consider:

- The LLM may describe a wall as "blocking movement in all directions" when it only blocks movement in two directions
- The LLM may miss a conditional constraint ("boxes can only be pushed onto targets") because the condition hasn't been triggered
- The LLM may have a correct description at a high level but be wrong about specific cell-level behavior

### 3.3 No Regression Testing

When the agent updates its understanding (e.g., discovers a new constraint), there is no way to verify that the updated model still explains all *previous* observations. A correction to the environment model might fix the latest observation while breaking consistency with observations from 30 actions ago.

### 3.4 Weak Error Signals

When the NL-JSON model is wrong, the only feedback is the LLM's next analysis marking `is_consistent_with_previous = false`. This binary signal lacks specificity:
- *Which* part of the model was wrong?
- *What* was predicted vs. what actually happened?
- *Where* on the grid did the prediction fail?

---

## 4. Proposed Architecture: Dual-Representation Integration

### 4.1 The Two Models

The system maintains **two parallel representations** of each game's domain model:

| Representation | Format | Purpose | Author |
|---------------|--------|---------|--------|
| **NL-JSON Model** (existing) | Structured natural language in Pydantic models | Semantic reasoning layer the LLM reasons over | LLM (via structured output) |
| **Python Symbolic Model** (proposed) | Executable Python subgame module | Verification and prediction layer | LLM (generates code from NL-JSON) |

The NL-JSON model is the **primary knowledge representation** — what the LLM reads, reasons about, and updates. The Python model is the **compiled, testable artifact** — generated from NL-JSON, executed to make predictions, and compared against ground truth.

### 4.2 High-Level Integration

```
                    ┌──────────────────────────────────────────────────────┐
                    │                  LearningAgent                       │
                    │              (Existing Orchestrator)                 │
                    └─────────────────────┬────────────────────────────────┘
                                          │
          ┌───────────────────────────────┼───────────────────────────────┐
          │                               │                               │
          ▼                               ▼                               ▼
  ┌───────────────┐             ┌─────────────────┐             ┌─────────────────┐
  │   LLM Calls   │             │  NL-JSON State  │             │  Python Model   │
  │  (3 per cycle) │◄───────────│  (AgentState)   │────────────►│ (SubgameModule) │
  │               │  updates    │                 │  compiles   │                 │
  │ 1.ActionAnaly │             │ ActionKnowledge │   to        │ GameObjects     │
  │ 2.EnvAnalysis │             │ EnvironmentKnow │             │ Transitions     │
  │ 3.NextAction  │             │ StageInfo       │             │ Physics         │
  │ ────────────  │             └────────┬────────┘             │ Constraints     │
  │ 4.Compile* ◄──┤                      │                      └────────┬────────┘
  │ 5.Diagnose*◄──┤                      │                               │
  └───────────────┘                      │                               │
                                         │                               │
                            ┌────────────▼───────────────────────────────▼──────┐
                            │              Verification Harness                 │
                            │                                                   │
                            │  For each (action, before_state, after_state)     │
                            │  in observation history:                          │
                            │    predicted = python_model.step(before, action)  │
                            │    actual    = after_state                        │
                            │    diff      = compare(predicted, actual)         │
                            │                                                   │
                            │  Returns: VerificationReport                      │
                            │    - per-cell diffs                               │
                            │    - which transition fired                       │
                            │    - which constraint applied/didn't apply        │
                            │    - regression failures                          │
                            └───────────────────────────────────────────────────┘
```

\* LLM calls 4 (Compile) and 5 (Diagnose) are new additions to the existing 3-call architecture.

### 4.3 Information Flow Per Cycle

```
1. Agent takes ACTION → observes (before_frame, after_frame)

2. Existing Phase 1: LLM analyzes action
   Input:  before/after images, diff, NL-JSON state
   Output: ActionAnalysisResult (interpretation, updated definition, consistency)

3. Existing Phase 1.5: LLM analyzes environment
   Input:  current frame, action context, NL-JSON state
   Output: EnvironmentAnalysisResult (objects, constraints, breakthroughs)

4. NEW — Predict with Python Model:
   predicted_after = subgame_model.step(before_frame, action)
   comparison = verify(predicted_after, actual_after)

5. NEW — If mismatch, generate structured error report:
   "transition move_up predicted cell (20,12)=0 but observed (20,12)=5
    constraint wall_collision did NOT fire when expected
    physics gravity was not applied after push"

6. NEW — Feed error report back into NL-JSON update:
   The LLM receives (observation, NL-JSON, Python model error report)
   and produces a more targeted update to the NL descriptions

7. NEW — Recompile Python model from updated NL-JSON:
   LLM translates updated NL-JSON → updated subgame module
   Verification harness replays ALL observation history to catch regressions

8. Existing Phase 3: LLM suggests next action
   (Now informed by Python model's prediction accuracy as additional context)
```

---

## 5. Python Game Engine Design

### 5.1 Two-Layer Architecture

The engine has two layers:

1. **Core Library** (domain-agnostic, human-authored, shared across all games)
2. **Per-Game Subgame Modules** (LLM-generated, evolved each iteration)

### 5.2 Core Library

The core library provides base classes and utilities that every subgame module uses. It is written once by a human and does not change per game.

#### 5.2.1 Grid / Board Representation

```python
class Grid:
    """NumPy-backed 2D grid with cell accessors and region queries.

    Wraps the raw ARC-AGI-3 frame data (64x64 integer grid) with
    convenient spatial operations.
    """

    def __init__(self, data: np.ndarray):
        self.data = data.copy()
        self.height, self.width = data.shape

    def get(self, row: int, col: int) -> int:
        """Get cell value, returns -1 for out-of-bounds."""

    def set(self, row: int, col: int, value: int) -> None:
        """Set cell value."""

    def in_bounds(self, row: int, col: int) -> bool:
        """Check if position is within grid bounds."""

    def neighbors(self, row: int, col: int, connectivity: int = 4) -> list[tuple[int, int]]:
        """Get valid neighbor positions (4 or 8 connectivity)."""

    def find_all(self, value: int) -> list[tuple[int, int]]:
        """Find all cells with a given value."""

    def region(self, min_row: int, min_col: int, max_row: int, max_col: int) -> np.ndarray:
        """Extract a rectangular region."""

    def diff(self, other: "Grid") -> list[tuple[int, int, int, int]]:
        """Cell-by-cell comparison. Returns [(row, col, self_val, other_val), ...]."""

    def snapshot(self) -> "Grid":
        """Deep copy for state comparison."""

    @classmethod
    def from_frame(cls, frame_data: list | np.ndarray) -> "Grid":
        """Construct from ARC-AGI-3 frame data (handles 2D and 3D)."""
```

#### 5.2.2 GameObject Base Class

```python
@dataclass
class GameObject:
    """Base class for game objects identified in the environment.

    The LLM subclasses this for each game's specific object types
    (e.g., Player, Wall, Box, Target, Key, Door).
    """

    object_id: str              # Unique identifier
    color: int                  # Grid color value (0-15)
    position: tuple[int, int]   # (row, col) of top-left corner
    width: int
    height: int
    properties: dict[str, Any]  # Game-specific properties

    def cells(self) -> list[tuple[int, int]]:
        """All cells this object occupies."""

    def overlaps(self, other: "GameObject") -> bool:
        """Check if this object overlaps with another."""

    def distance_to(self, other: "GameObject") -> float:
        """Euclidean distance between centers."""

    def adjacent_to(self, other: "GameObject", direction: str | None = None) -> bool:
        """Check adjacency (optionally in a specific direction)."""
```

#### 5.2.3 GameState Container

```python
@dataclass
class GameState:
    """Complete state of the game at a point in time.

    Holds the grid, all instantiated objects, and metadata.
    Used as the input/output type for transition functions.
    """

    grid: Grid                          # Current grid state
    objects: dict[str, GameObject]      # All game objects by ID
    step: int                           # Action count
    score: int                          # Current score
    action_history: list[str]           # Actions taken so far
    metadata: dict[str, Any]            # Game-specific metadata

    @classmethod
    def from_grid(cls, grid: Grid, object_registry: "ObjectRegistry") -> "GameState":
        """Parse a grid into a GameState by detecting objects using the registry."""

    def snapshot(self) -> "GameState":
        """Deep copy for prediction comparison."""

    def apply(self, new_grid: Grid) -> "GameState":
        """Create new state with updated grid, re-detecting objects."""
```

#### 5.2.4 Object Registry

```python
class ObjectRegistry:
    """Maps grid patterns to GameObject subclasses.

    Each subgame module registers its object types here.
    The registry is used by GameState.from_grid() to parse
    a raw grid into typed game objects.
    """

    def register(self, color: int, cls: type[GameObject],
                 matcher: Callable[[Grid, DetectedObject], bool] | None = None) -> None:
        """Register an object type. Matcher is optional custom logic."""

    def parse_grid(self, grid: Grid) -> dict[str, GameObject]:
        """Detect and instantiate all objects from a grid."""
```

#### 5.2.5 Action Interface

```python
class Action(str, Enum):
    """ARC-AGI-3 actions, mirroring the existing ActionID enum."""
    ACTION1 = "ACTION1"
    ACTION2 = "ACTION2"
    ACTION3 = "ACTION3"
    ACTION4 = "ACTION4"
    ACTION5 = "ACTION5"
```

#### 5.2.6 SubgameModule Protocol

```python
class SubgameModule(Protocol):
    """Protocol that every LLM-generated subgame module must satisfy.

    The core library's verification harness calls these methods.
    """

    def create_registry(self) -> ObjectRegistry:
        """Define game object types and how to detect them on the grid."""

    def transition(self, state: GameState, action: Action) -> GameState:
        """Apply an action: run the action's transition, then physics, then constraints.
        Returns the predicted next state."""

    def get_transition_log(self) -> list[str]:
        """Return a log of which transitions, physics, and constraints fired
        during the last call to transition(). Used for error diagnosis."""
```

#### 5.2.7 Verification Harness

```python
@dataclass
class PredictionResult:
    """Result of comparing a predicted state against an observed state."""

    step: int
    action: Action
    predicted_grid: Grid
    actual_grid: Grid
    cell_diffs: list[tuple[int, int, int, int]]  # (row, col, predicted, actual)
    is_correct: bool
    transition_log: list[str]  # Which transitions/physics/constraints fired


@dataclass
class VerificationReport:
    """Full report from replaying observation history against the Python model."""

    total_steps: int
    correct_predictions: int
    failed_predictions: int
    failures: list[PredictionResult]
    regression_failures: list[PredictionResult]  # Failures on previously-correct steps
    accuracy: float  # correct / total


class VerificationHarness:
    """Replays observation history against a SubgameModule to verify correctness."""

    def verify_single(self, module: SubgameModule,
                      before_grid: Grid, action: Action,
                      actual_after_grid: Grid) -> PredictionResult:
        """Predict one step and compare against actual."""

    def verify_history(self, module: SubgameModule,
                       observations: list[ObservationRecord]) -> VerificationReport:
        """Replay all observations, producing a full verification report.

        Each observation is independently verified (before → action → predicted
        vs. actual after). This catches both current errors and regressions.
        """

    def format_error_for_llm(self, failure: PredictionResult) -> str:
        """Format a prediction failure into a structured error message
        suitable for inclusion in the LLM's next prompt.

        Example output:
          Step 12: ACTION1 (move_up)
          PREDICTION FAILED at 8 cells:
            (20, 12): predicted 0 (empty), actual 5 (wall) — wall_collision constraint missed
            (16, 12): predicted 2 (player), actual 0 (empty) — player did not move here
          Transition log: move_up fired, gravity NOT applied, wall_collision NOT checked
          Likely cause: wall at (20,12) was not registered in the object model
        """
```

### 5.3 Per-Game Subgame Module Structure

Each game's subgame module is a Python file with four sections, all generated and iteratively refined by the LLM from the NL-JSON state. The structure maps directly onto the existing `EnvironmentKnowledge` and `ActionKnowledge` representations.

#### 5.3.1 Section 1: Object Encoding

Maps directly from `EnvironmentKnowledge.identified_objects`:

```python
# === SECTION 1: OBJECT ENCODING ===
# Source: EnvironmentKnowledge.identified_objects + breakthroughs

class Player(GameObject):
    """The player character.
    NL source: 'red (color 2) 4x4 block, responds to ACTION1-4'
    """
    pass

class Wall(GameObject):
    """Impassable wall blocks.
    NL source: 'grey (color 5) blocks, form border and internal maze'
    """
    pass

class PushableBox(GameObject):
    """Boxes that can be pushed by the player.
    NL source: 'orange (color 8) 4x4 block, moves when player pushes into it'
    """
    is_on_target: bool = False

class Target(GameObject):
    """Target positions for boxes.
    NL source: 'yellow (color 7) 4x4 marker, destination for boxes'
    """
    pass

def create_registry() -> ObjectRegistry:
    registry = ObjectRegistry()
    registry.register(color=2, cls=Player)
    registry.register(color=5, cls=Wall)
    registry.register(color=8, cls=PushableBox)
    registry.register(color=7, cls=Target)
    return registry
```

#### 5.3.2 Section 2: Transition Functions

Maps directly from `ActionKnowledge.current_definition`:

```python
# === SECTION 2: TRANSITIONS ===
# Source: ActionKnowledge[ACTION1-5].current_definition

def transition_action1(state: GameState) -> GameState:
    """ACTION1: Move player up.
    NL source: 'Moves player up by one tile. If wall above, no movement.
                If pushable box above with empty space above it, both move up.'
    """
    player = state.objects.get("player")
    if not player:
        return state

    target_row = player.position[0] - 4  # One tile up (4px grid cells)
    target_col = player.position[1]

    # Check what is at the target position
    target_cell_value = state.grid.get(target_row, target_col)

    if target_cell_value == 5:  # Wall
        _log("move_up blocked by wall")
        return state  # No movement

    if target_cell_value == 8:  # Pushable box
        box_target_row = target_row - 4
        box_target_value = state.grid.get(box_target_row, target_col)
        if box_target_value in (0, 7):  # Empty or target
            _log("move_up: pushing box")
            # Move box up
            _move_object(state, "box", box_target_row, target_col)
            # Move player up
            _move_object(state, "player", target_row, target_col)
            return state
        else:
            _log("move_up blocked: box cannot be pushed (obstruction)")
            return state  # Box is blocked

    if target_cell_value == 0:  # Empty space
        _log("move_up: player moves freely")
        _move_object(state, "player", target_row, target_col)

    return state

# Similar functions for ACTION2-5...
```

#### 5.3.3 Section 3: Physics / Utilities

Maps from `EnvironmentKnowledge.spatial_rules` and `movement_constraints`:

```python
# === SECTION 3: PHYSICS / UTILITIES ===
# Source: EnvironmentKnowledge.movement_constraints + spatial_rules + breakthroughs

def apply_gravity(state: GameState) -> GameState:
    """Apply gravity: unsupported objects fall to lowest open cell.
    NL source: breakthrough 'Objects fall when unsupported'
    Only applies if gravity is a discovered mechanic for this game.
    """
    for obj_id, obj in state.objects.items():
        if isinstance(obj, PushableBox):
            # Find lowest empty row below current position
            lowest_row = obj.position[0]
            for row in range(obj.position[0] + 4, state.grid.height - 4, 4):
                if state.grid.get(row, obj.position[1]) == 0:
                    lowest_row = row
                else:
                    break
            if lowest_row != obj.position[0]:
                _move_object(state, obj_id, lowest_row, obj.position[1])
    return state

def check_win_condition(state: GameState) -> bool:
    """Check if all boxes are on targets.
    NL source: domain_description 'Sokoban-like puzzle'
    """
    boxes = [o for o in state.objects.values() if isinstance(o, PushableBox)]
    targets = [o for o in state.objects.values() if isinstance(o, Target)]
    for target in targets:
        if not any(b.position == target.position for b in boxes):
            return False
    return True
```

#### 5.3.4 Section 4: Constraints

Maps from `EnvironmentKnowledge.movement_constraints` and contextual observations:

```python
# === SECTION 4: CONSTRAINTS ===
# Source: ActionObservation.context_that_caused_outcome patterns
#         EnvironmentKnowledge.movement_constraints

def constraint_move_budget(state: GameState, action: Action) -> GameState | None:
    """Move budget constraint: UI dots count remaining moves.
    NL source: breakthrough 'Red dots at top decrease with each effective move.
               When all dots are gone, game resets.'

    Returns None to allow the transition to proceed normally.
    Returns a modified GameState to override the transition result.
    """
    if state.metadata.get("moves_remaining", float("inf")) <= 0:
        _log("constraint: move budget exhausted, action blocked")
        return state  # No-op: out of moves
    return None  # Allow transition to proceed

def constraint_boundary(state: GameState, action: Action) -> GameState | None:
    """Boundary constraint: nothing can move outside the border.
    NL source: 'Color 5 grey border surrounds the play area'
    """
    # The transition functions already check for walls,
    # but this is a safety constraint that catches edge cases
    return None
```

#### 5.3.5 Top-Level Transition Orchestrator

```python
# === ORCHESTRATOR ===
# Combines all sections into the SubgameModule protocol

TRANSITION_MAP = {
    Action.ACTION1: transition_action1,  # Move up
    Action.ACTION2: transition_action2,  # Move down
    Action.ACTION3: transition_action3,  # Move left
    Action.ACTION4: transition_action4,  # Move right
    Action.ACTION5: transition_action5,  # Interact
}

PHYSICS = [
    apply_gravity,          # Only if gravity is discovered
]

CONSTRAINTS = [
    constraint_move_budget,
    constraint_boundary,
]

_transition_log: list[str] = []

def _log(msg: str):
    _transition_log.append(msg)

def transition(state: GameState, action: Action) -> GameState:
    """Full transition: constraints → action → physics → constraints."""
    _transition_log.clear()

    # Pre-transition constraints (may block the action entirely)
    for constraint in CONSTRAINTS:
        override = constraint(state, action)
        if override is not None:
            return override

    # Apply action-specific transition
    transition_fn = TRANSITION_MAP.get(action)
    if transition_fn:
        state = transition_fn(state)
        _log(f"transition {action.value} applied")
    else:
        _log(f"no transition for {action.value}")

    # Apply physics
    for physics_fn in PHYSICS:
        state = physics_fn(state)

    # Post-transition constraints
    for constraint in CONSTRAINTS:
        override = constraint(state, action)
        if override is not None:
            return override

    return state

def get_transition_log() -> list[str]:
    return list(_transition_log)
```

---

## 6. Integration with the LearningAgent Lifecycle

### 6.1 Where the Game Engine Fits in the Existing Phases

The Python game engine introduces two new sub-phases that slot into the existing pipeline without disrupting it. The existing three LLM calls remain unchanged; two new calls are added conditionally.

```
EXISTING PHASE 1: Action Analysis (LLM Call 1)
  → Returns ActionAnalysisResult
  → Updates ActionKnowledge via KnowledgeManager.update_from_analysis()

EXISTING PHASE 1.5: Environment Analysis (LLM Call 2)
  → Returns EnvironmentAnalysisResult
  → Updates EnvironmentKnowledge via KnowledgeManager.update_environment_from_analysis()

  ╔══════════════════════════════════════════════════════════════════╗
  ║ NEW PHASE 1.7: Python Model Prediction & Verification          ║
  ║                                                                ║
  ║   a) If subgame_module exists:                                 ║
  ║      predicted = subgame_module.transition(before_state, action)║
  ║      result = verification_harness.verify_single(              ║
  ║          subgame_module, before_grid, action, actual_after_grid)║
  ║                                                                ║
  ║   b) If prediction correct:                                    ║
  ║      Increment model_correct_count on ActionKnowledge          ║
  ║      (Stronger verification signal than LLM consistency alone) ║
  ║                                                                ║
  ║   c) If prediction incorrect:                                  ║
  ║      error_report = harness.format_error_for_llm(result)       ║
  ║      Store error_report for use in next compile step           ║
  ╚══════════════════════════════════════════════════════════════════╝

  ╔══════════════════════════════════════════════════════════════════╗
  ║ NEW PHASE 1.9: Recompile Python Model (LLM Call 4, periodic)  ║
  ║                                                                ║
  ║   Triggered when:                                              ║
  ║     - NL-JSON model was updated (new definition or env change) ║
  ║     - AND (no Python model exists yet                          ║
  ║           OR prediction was incorrect                          ║
  ║           OR every N actions as a refresh)                     ║
  ║                                                                ║
  ║   Input to LLM:                                                ║
  ║     - Current NL-JSON state (all ActionKnowledge + Environment)║
  ║     - Core library API reference (base classes + methods)      ║
  ║     - Previous subgame module code (if any)                    ║
  ║     - Error report from verification (if any)                  ║
  ║     - Full observation history for regression context          ║
  ║                                                                ║
  ║   Output from LLM:                                             ║
  ║     - Updated Python subgame module code                       ║
  ║                                                                ║
  ║   Post-compilation:                                            ║
  ║     regression_report = harness.verify_history(                ║
  ║         new_module, all_observations)                          ║
  ║     If regressions introduced: log them, optionally re-compile ║
  ╚══════════════════════════════════════════════════════════════════╝

EXISTING PHASE 3: Next Action Suggestion (LLM Call 3)
  → Now includes Python model accuracy as additional context:
    "Python model accuracy: 85% (17/20 correct predictions)"
    "Last failure: ACTION1 at step 14 — wall constraint missed"

EXISTING PHASES 4/5: Execute Action
  → No changes
```

### 6.2 New Fields on Existing Models

Minimal additions to the existing Pydantic models:

```python
# Addition to ActionKnowledge (models.py)
class ActionKnowledge(BaseModel):
    # ... existing fields ...
    model_correct_predictions: int = 0    # Python model predicted correctly
    model_incorrect_predictions: int = 0  # Python model predicted incorrectly
    last_model_error: str | None = None   # Most recent prediction error detail

# Addition to AgentState (models.py)
class AgentState(BaseModel):
    # ... existing fields ...
    subgame_module_code: str | None = None       # Current Python model source
    subgame_module_version: int = 0              # Increments on each recompile
    model_verification_accuracy: float = 0.0     # Overall prediction accuracy
    last_verification_report: str | None = None  # Summary of last full verification
```

### 6.3 New KnowledgeManager Methods

```python
# Addition to KnowledgeManager (knowledge.py)

def format_for_model_compilation(
    self, state: AgentState, error_report: str | None = None
) -> dict[str, str]:
    """Format all knowledge for the LLM model compilation prompt.

    Returns a dict with:
    - 'nl_state': Full NL-JSON state formatted for the compilation prompt
    - 'core_api': Reference for core library base classes
    - 'previous_code': Previous subgame module code (if any)
    - 'error_report': Structured error from last verification failure
    - 'observation_history': All observations for regression context
    """

def update_from_verification(
    self, state: AgentState, action_id: ActionID,
    prediction_result: PredictionResult
) -> AgentState:
    """Update state based on Python model prediction result.

    Increments model_correct_predictions or model_incorrect_predictions
    on the relevant ActionKnowledge.
    """
```

---

## 7. The Compilation Step: NL-JSON to Python Model

### 7.1 The Compilation Prompt

The compilation step is a new (4th) LLM call that translates the current NL-JSON understanding into executable Python code. This is conceptually similar to asking the LLM to "write a unit test" for its own understanding.

**System prompt** (new, `llm_agents.py`):

```
You are a game model compiler. Your job is to translate a natural language
description of a game's mechanics into an executable Python simulation.

You will receive:
1. A structured description of all game objects, their properties and roles
2. Definitions of what each action does
3. Environment constraints and physics rules
4. A core library API that provides Grid, GameObject, GameState, and Action classes
5. Optionally: the previous version of the model and an error report

Your output must be a valid Python module that implements the SubgameModule protocol:
- create_registry(): Register all object types
- transition(state, action): Apply action with physics and constraints
- get_transition_log(): Return what happened during the last transition

CRITICAL RULES:
- Every claim in the NL description must be translated to executable code
- If the NL description is ambiguous, make the most conservative assumption
- Include _log() calls for every decision point (wall check, push check, etc.)
- The model must be deterministic: same state + same action = same result
- Use the core library's Grid, GameObject, and GameState — do not reimplement them
```

**User prompt structure**:

```
GAME OBJECTS (from EnvironmentKnowledge):
{formatted_identified_objects}

ACTION DEFINITIONS (from ActionKnowledge):
{formatted_action_definitions}

ENVIRONMENT CONSTRAINTS (from EnvironmentKnowledge):
{formatted_movement_constraints}
{formatted_internal_walls}
{formatted_spatial_rules}

BREAKTHROUGHS / PHYSICS RULES:
{formatted_breakthroughs}

DOMAIN DESCRIPTION:
{domain_description}

CORE LIBRARY API:
{core_library_reference}

PREVIOUS MODEL (version {version}):
{previous_subgame_module_code_or_none}

ERROR REPORT FROM LAST VERIFICATION:
{error_report_or_none}

Generate the complete Python subgame module.
```

### 7.2 When to Compile

Not every cycle needs a recompilation. The compilation trigger conditions:

| Condition | Compile? | Rationale |
|-----------|----------|-----------|
| No Python model exists yet AND at least 3 actions observed | Yes | Need minimum observations before first model |
| NL-JSON updated AND previous Python prediction was wrong | Yes | Both signals indicate model needs fixing |
| NL-JSON updated AND it's been >5 actions since last compile | Yes | Periodic refresh to incorporate accumulated changes |
| NL-JSON NOT updated AND prediction was correct | No | Model is working, don't waste an LLM call |
| Stage transition detected | Yes | New stage may change rules entirely |

### 7.3 Compilation Output Handling

The LLM's output is a Python source string. The system:

1. **Receives** the code as structured output (or a code block extraction)
2. **Validates** syntax with `ast.parse()` (no execution yet)
3. **Loads** as a module via `importlib` in a sandboxed namespace
4. **Verifies** the module satisfies the `SubgameModule` protocol (has required methods)
5. **Runs** the verification harness against all observation history
6. **Stores** the source in `AgentState.subgame_module_code` if validation passes
7. **Falls back** to the previous version if the new code has syntax errors or runtime crashes

---

## 8. Verification Harness and Feedback Loop

### 8.1 Single-Step Verification

For each action the agent takes:

```
Input:
  before_grid: Grid from before_frame (stored in pending_analysis)
  action: The action that was taken
  actual_after_grid: Grid from the actual API response

Process:
  1. Parse before_grid into GameState using subgame_module.create_registry()
  2. predicted_state = subgame_module.transition(game_state, action)
  3. cell_diffs = predicted_state.grid.diff(actual_after_grid)
  4. transition_log = subgame_module.get_transition_log()

Output:
  PredictionResult:
    is_correct: len(cell_diffs) == 0
    cell_diffs: [(row, col, predicted_val, actual_val), ...]
    transition_log: ["move_up fired", "wall_collision checked", ...]
```

### 8.2 History Replay (Regression Testing)

After every recompilation, the harness replays the **entire observation history**:

```python
# Every ActionObservation has before_frame_path and after_frame_path
# These are loaded, and the model is tested against each one

observations = collect_all_observations(state.action_knowledge)
report = harness.verify_history(new_module, observations)

# report.accuracy = correct / total
# report.regression_failures = steps that WERE correct before but now FAIL
```

This ensures that fixing one transition doesn't break another. Regression failures are prominently highlighted in the next compilation prompt.

### 8.3 Error Signal Format

The error signal fed back to the LLM is structured for maximum diagnostic value:

```
═══════════════════════════════════════════════════
PYTHON MODEL VERIFICATION FAILURE — Step 14, ACTION1
═══════════════════════════════════════════════════

PREDICTION vs ACTUAL (8 cells differ):
  Cell (20, 12): predicted=0 (empty), actual=5 (wall)
  Cell (16, 12): predicted=2 (player), actual=0 (empty)
  Cell (20, 16): predicted=0 (empty), actual=2 (player)
  ...

TRANSITION LOG:
  1. move_up fired
  2. target cell (16,12) checked: value=0 → classified as EMPTY
  3. player moved to (16,12)
  4. gravity NOT applied (not registered)
  5. wall_collision NOT checked for cell (20,12)

LIKELY ISSUE:
  The model moved the player to (16,12) but the actual player ended up at (20,16).
  Cell (20,12) was predicted empty but is actually a wall (color 5).
  → Possible causes:
    a) A wall at (20,12) was not registered in the object model
    b) The transition checked the wrong cell for collision
    c) A constraint should have redirected the movement

NL-JSON DEFINITION FOR ACTION1:
  "Moves player up by one tile, blocked by grey walls"

RELEVANT ENVIRONMENT KNOWLEDGE:
  Internal walls: ["Grey blocks at rows 20-24 form a horizontal wall"]
  → This wall IS described in the NL model but NOT implemented in the Python model
═══════════════════════════════════════════════════
```

### 8.4 Enhanced Verification vs. Existing Verification

The Python model verification **complements** the existing LLM consistency checking, not replaces it:

| Dimension | Existing (LLM Consistency) | New (Python Model Prediction) |
|-----------|---------------------------|-------------------------------|
| What's tested | "Does the LLM describe this consistently?" | "Does the model correctly predict outcomes?" |
| Signal type | Binary (consistent / inconsistent) | Structured (which cells, which transitions) |
| Ground truth | LLM's own opinion | Actual ARC-AGI-3 API observations |
| Regression testing | None | Full history replay |
| Speed | Requires LLM call | Instant (Python execution) |
| Failure diagnosis | "The description was wrong" | "Cell (20,12) predicted 0, actual 5, wall_collision not fired" |

Both signals are valuable. The LLM consistency check catches high-level semantic issues ("this action seems to do something completely different than I thought"). The Python prediction check catches precise implementation issues ("the wall collision logic is missing for this specific wall configuration").

---

## 9. Concrete Data Flow: A Worked Example

### Step 1: Agent Takes ACTION1

```
Before frame: Player (color 2) at position (24, 12). Wall (color 5) at (20, 12).
Agent executes ACTION1 (move up).
After frame: Player still at (24, 12). No change — wall blocked movement.
```

### Step 2: Existing Phase 1 — Action Analysis (LLM Call 1)

```json
{
  "interpretation": "ACTION1 attempted to move the player up but was blocked by a wall",
  "had_effect": false,
  "is_consistent_with_previous": true,
  "context_that_caused_this_outcome": "Wall at row 20 blocks upward movement",
  "new_definition": null,
  "update_definition": false
}
```

### Step 3: Existing Phase 1.5 — Environment Analysis (LLM Call 2)

```json
{
  "movement_constraints": ["Player blocked by grey wall at row 20 when moving up"],
  "breakthroughs": [],
  "objects_identified": [
    {"name": "player", "color": "2", "role_hypothesis": "Player character"},
    {"name": "wall_barrier", "color": "5", "role_hypothesis": "Impassable wall"}
  ]
}
```

### Step 4: New Phase 1.7 — Python Model Prediction

```
subgame_module.transition(before_state, ACTION1):
  transition_action1 fires:
    target cell (20, 12) = 5 (wall)
    _log("move_up blocked by wall")
    return state unchanged

predicted_grid == actual_grid → CORRECT ✓
model_correct_predictions for ACTION1 incremented to 6
```

No recompilation needed. The Python model correctly predicted the outcome.

### Step 5 (alternate): What If the Prediction Was Wrong?

Say the Python model had a bug — it didn't know about an internal wall and predicted the player would move:

```
predicted_grid: player at (20, 12) — model thought player would move up
actual_grid: player at (24, 12) — player stayed put

Error report generated:
  Cell (24, 12): predicted=0, actual=2 (player should still be here)
  Cell (20, 12): predicted=2, actual=5 (wall exists here, model didn't know)
  Transition log: "move_up fired, target (20,12) classified as EMPTY, player moved"
  Likely issue: wall at (20,12) not in object registry

→ This error report is fed into Phase 1.9 (recompile)
→ The LLM sees exactly which wall was missing and generates updated code
→ Regression harness confirms the fix doesn't break other predictions
```

---

## 10. Implementation Roadmap

### Phase 1: Core Library (No LLM Changes)

Build the human-authored core library:
- `engine/grid.py`: Grid class with cell ops, diff, snapshot
- `engine/objects.py`: GameObject base, ObjectRegistry
- `engine/state.py`: GameState container
- `engine/harness.py`: VerificationHarness, PredictionResult, VerificationReport
- `engine/__init__.py`: Public API

Deliverable: A testable library that can load a Grid from ARC-AGI-3 frame data and run a SubgameModule against it.

### Phase 2: Verification Integration (Minimal LLM Changes)

Add Python model prediction to the existing lifecycle:
- Add `model_correct_predictions` / `model_incorrect_predictions` to `ActionKnowledge`
- Add `subgame_module_code` and `model_verification_accuracy` to `AgentState`
- Add Phase 1.7 (predict + verify) to `_complete_action_analysis()` in `agent.py`
- Add `format_error_for_llm()` output to the existing LLM prompts as optional context
- Store `ObservationRecord` entries (before_grid, action, after_grid) for replay

Deliverable: The agent can load a manually-written subgame module and verify it against observations.

### Phase 3: LLM Compilation (New LLM Call)

Add the compilation step:
- Add LLM Call 4 (compile) to `llm_agents.py` with system prompt and JSON schema
- Add compilation trigger logic to `agent.py` (Phase 1.9)
- Add `format_for_model_compilation()` to `KnowledgeManager`
- Add code validation (syntax check, protocol check, sandbox execution)
- Add regression testing after each recompilation

Deliverable: The agent autonomously generates and iterates on Python subgame modules from its NL-JSON understanding.

### Phase 4: Diagnostic Feedback Loop (New LLM Call)

Add targeted diagnosis when predictions fail:
- Add LLM Call 5 (diagnose) that receives the error report and suggests NL-JSON corrections
- Close the loop: Python error → NL-JSON correction → recompile → re-verify
- Add the Python model's accuracy to the next-action suggestion prompt (Phase 3)

Deliverable: Full closed-loop dual-representation system where Python errors drive NL-JSON corrections and vice versa.

### Phase 5: Advanced Features

- **Partial models**: Allow the Python model to represent only *known* mechanics and return `UNKNOWN` for unmodeled transitions (rather than requiring a complete model)
- **Hypothesis testing**: Use the Python model to generate *targeted experiments* — "if my model is correct, ACTION3 from position (32, 40) should move the player left to (32, 36); let me test this"
- **Model merging across stages**: When a stage transition occurs, diff the old and new models to identify what changed
- **Action budget optimization**: Use the Python model to plan action sequences that maximize information gain per action

---

## 11. Why Python Is the Right Modeling Language

The original draft established the case thoroughly; here we summarize the key points in the context of this integration:

### 11.1 LLM Code Generation Quality

- LLMs default to Python 90-97% of the time on language-agnostic tasks (Twist et al., 2025)
- Python ranks #1 on MultiPL-E, BabelCode/TP3, MBXP, and HumanEval-X (Cassano et al., 2023)
- PDDL, RDDL, and custom DSLs have zero benchmark coverage
- LLMs produce more syntax errors in DSLs than in Python (Liu et al., ICLR 2025)

### 11.2 ARC-Specific Precedent

- SOAR (ICML 2025): 52% on ARC using Python program synthesis
- Pythonic-DSL (ICLR 2025): Python output had higher executable program rate than DSL output
- Every top ARC result uses Python; none use PDDL or RDDL

### 11.3 Compatibility with the LearningAgent

- The agent already uses Python Pydantic models for state representation
- NumPy is already a dependency (used in diff.py, object_detection.py)
- The OpenAI structured output API can return code blocks
- The subgame module pattern (register objects, define transitions) maps directly to the existing NL-JSON structure
- Python's `importlib` enables runtime loading of LLM-generated modules

### 11.4 Expressiveness for ARC-AGI-3

ARC-AGI-3 games require modeling:
- Grid manipulation (NumPy native)
- Object identity and tracking (Python classes with properties)
- Conditional transitions (if/else logic)
- Physics rules (iteration, flood fill, propagation)
- State-dependent constraints (game state inspection)
- None of which PDDL/RDDL can express; all of which Python handles natively

---

## 12. References

[1] Twist, L., Zhang, J.M., Harman, M., Syme, D. et al. (2025). "LLMs Love Python: A Study of LLMs' Bias for Programming Languages and Libraries." arXiv:2503.17181.

[2] Pourcel, J., Colas, C., Oudeyer, P.-Y. (2025). "Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI." ICML 2025. (SOAR)

[3] Liu, M. et al. (2025). "Synthesizing Programmatic Reinforcement Learning Policies with Large Language Model Guided Search." ICLR 2025. (LLM-GS / Pythonic-DSL)

[4] Cassano, F. et al. (2023). "MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation." NeurIPS 2023.

[5] Chen, M. et al. (2021). "Evaluating Large Language Models Trained on Code." OpenAI. arXiv:2107.03374. (HumanEval)

[6] ARC Prize Foundation (2025). "ARC Prize 2025: Technical Report." arXiv:2601.10904.

[7] Greenblatt, R. (2024). ARC-solution documentation. GPT-4o k=2048 program sampling approach.

[8] Khalil, E.B. et al. (2024). "LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations." TMLR 2024.

---

*This document describes the integration framework for combining the existing LearningAgent's NL-JSON pipeline with a Python symbolic game engine. The core insight is that these representations are complementary: NL-JSON is what the LLM reasons over; Python is what the system executes and tests against ground truth. Together, they close the verification loop that NL-JSON alone cannot provide.*